[
  {
    "objectID": "posts/Lab02.html",
    "href": "posts/Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/sxj210076/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'MASS'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\sxj210076\\AppData\\Local\\R\\win-library\\4.3\\00LOCK\\MASS\\libs\\x64\\MASS.dll\nto C:\\Users\\sxj210076\\AppData\\Local\\R\\win-library\\4.3\\MASS\\libs\\x64\\MASS.dll:\nPermission denied\n\n\nWarning: restored 'MASS'\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\sxj210076\\AppData\\Local\\Temp\\Rtmpg3nmXz\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.3.3\n\n\nLoading required package: ISLR\n\n\nWarning: package 'ISLR' was built under R version 4.3.3\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Lab02.html#indexing-data-using",
    "href": "posts/Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "posts/Lab02.html#loading-data-from-github-remote",
    "href": "posts/Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "posts/Lab02.html#load-data-from-islr-website",
    "href": "posts/Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "posts/Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "posts/Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "posts/Lab02.html#linear-regression",
    "href": "posts/Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/sxj210076/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'MASS'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\sxj210076\\AppData\\Local\\R\\win-library\\4.3\\00LOCK\\MASS\\libs\\x64\\MASS.dll\nto C:\\Users\\sxj210076\\AppData\\Local\\R\\win-library\\4.3\\MASS\\libs\\x64\\MASS.dll:\nPermission denied\n\n\nWarning: restored 'MASS'\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\sxj210076\\AppData\\Local\\Temp\\Rtmpg3nmXz\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.3.3\n\n\nLoading required package: ISLR\n\n\nWarning: package 'ISLR' was built under R version 4.3.3\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "posts/Lab02.html#multiple-linear-regression",
    "href": "posts/Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "posts/Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "posts/Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Lab02.html#qualitative-predictors",
    "href": "posts/Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "posts/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "posts/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/assignment1.html",
    "href": "posts/assignment1.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Code\nlibrary(haven)\nlibrary (ggplot2)\nTEDS_2016 &lt;- read_dta(\"C:/Users/sxj210076/Downloads/TEDS_2016.dta\")\nhist(TEDS_2016$age, col = \"indianred4\")\n\n\n\n\n\nCode\nhist(TEDS_2016$income, col = \"steelblue4\")\n\n\n\n\n\nCode\nggplot(TEDS_2016) + \n  geom_bar(mapping = aes(x = edu),fill = \"indianred4\")\n\n\n\n\n\nCode\nsummary(TEDS_2016$age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.00   35.00   49.00   49.11   61.00  100.00 \n\n\n\n\nCode\n# Generate a frequency table of the \"Tondu\" variable} \nfrequency_table &lt;- table(TEDS_2016$Tondu) \nprint(frequency_table)\n\n\n\n  1   2   3   4   5   6   9 \n 27 180 546 328 380 108 121 \n\n\n\n\n\n\n\nCode\nlibrary (ggplot2)\nlibrary(dplyr)\n\n# Convert Tondu variable to factor with corresponding labels\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, labels = \n                            c(\"Unification now\", \"Status quo, unif. in future\", \n                              \"Status quo, decide later\", \"Status quo forever\",\n                              \"Status quo, indep. in future\", \"Independence now\", \n                              \"No response\"), exclude = NA)\nggplot(TEDS_2016, aes(x = Tondu)) +\n  geom_bar(fill = \"darkslategrey\") +\n  labs(title = \"Position on Unification and Independence\") +\n  coord_flip()\n\n\n\n\n\nCode\nggplot(data = TEDS_2016) +\n  geom_boxplot(mapping = aes(x = reorder(Tondu, age, FUN = median), y = age), fill = \"indianred4\") +\n  coord_flip()\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Taiwanese))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"steelblue4\", \"1\" = \"indianred4\")) + coord_flip()\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(DPP))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"steelblue4\", \"1\" = \"indianred4\")) + coord_flip()\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(female))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"steelblue4\", \"1\" = \"indianred4\")) + coord_flip()\n\n\n\n\n\n\n\n\n\n\nCode\nTEDS_2016 &lt;- read_dta(\"C:/Users/sxj210076/Downloads/TEDS_2016.dta\")\nTEDS_2016$votetsai &lt;- factor(TEDS_2016$votetsai, labels = \n                               c(\"No\", \"Yes\"))\nTEDS_2016 &lt;- na.omit(TEDS_2016)\nggplot(TEDS_2016, aes(x = votetsai)) +\n  geom_bar(fill = \"darkslategrey\") +\n  labs(title = \"Vote for DPP Candidate Tsai Ing-wen\")\n\n\n\n\n\nCode\nggplot(data = TEDS_2016) +\n  geom_boxplot(mapping = aes(x = votetsai, y = age), fill = \"indianred4\") +\n  coord_flip()\n\n\n\n\n\nCode\nggplot(data = TEDS_2016) +\n  geom_boxplot(mapping = aes(x = votetsai, y = income), fill = \"indianred4\") +\n  coord_flip()\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = votetsai, fill = factor(Taiwanese))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"steelblue4\", \"1\" = \"indianred4\"))\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = edu, fill = factor(votetsai))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"No\" = \"steelblue4\", \"Yes\" = \"indianred4\"))"
  },
  {
    "objectID": "posts/assignment1.html#questions",
    "href": "posts/assignment1.html#questions",
    "title": "Assignment 2",
    "section": "",
    "text": "Code\nlibrary(haven)\nlibrary (ggplot2)\nTEDS_2016 &lt;- read_dta(\"C:/Users/sxj210076/Downloads/TEDS_2016.dta\")\nhist(TEDS_2016$age, col = \"indianred4\")\n\n\n\n\n\nCode\nhist(TEDS_2016$income, col = \"steelblue4\")\n\n\n\n\n\nCode\nggplot(TEDS_2016) + \n  geom_bar(mapping = aes(x = edu),fill = \"indianred4\")\n\n\n\n\n\nCode\nsummary(TEDS_2016$age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.00   35.00   49.00   49.11   61.00  100.00 \n\n\n\n\nCode\n# Generate a frequency table of the \"Tondu\" variable} \nfrequency_table &lt;- table(TEDS_2016$Tondu) \nprint(frequency_table)\n\n\n\n  1   2   3   4   5   6   9 \n 27 180 546 328 380 108 121 \n\n\n\n\n\n\n\nCode\nlibrary (ggplot2)\nlibrary(dplyr)\n\n# Convert Tondu variable to factor with corresponding labels\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, labels = \n                            c(\"Unification now\", \"Status quo, unif. in future\", \n                              \"Status quo, decide later\", \"Status quo forever\",\n                              \"Status quo, indep. in future\", \"Independence now\", \n                              \"No response\"), exclude = NA)\nggplot(TEDS_2016, aes(x = Tondu)) +\n  geom_bar(fill = \"darkslategrey\") +\n  labs(title = \"Position on Unification and Independence\") +\n  coord_flip()\n\n\n\n\n\nCode\nggplot(data = TEDS_2016) +\n  geom_boxplot(mapping = aes(x = reorder(Tondu, age, FUN = median), y = age), fill = \"indianred4\") +\n  coord_flip()\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Taiwanese))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"steelblue4\", \"1\" = \"indianred4\")) + coord_flip()\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(DPP))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"steelblue4\", \"1\" = \"indianred4\")) + coord_flip()\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(female))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"steelblue4\", \"1\" = \"indianred4\")) + coord_flip()\n\n\n\n\n\n\n\n\n\n\nCode\nTEDS_2016 &lt;- read_dta(\"C:/Users/sxj210076/Downloads/TEDS_2016.dta\")\nTEDS_2016$votetsai &lt;- factor(TEDS_2016$votetsai, labels = \n                               c(\"No\", \"Yes\"))\nTEDS_2016 &lt;- na.omit(TEDS_2016)\nggplot(TEDS_2016, aes(x = votetsai)) +\n  geom_bar(fill = \"darkslategrey\") +\n  labs(title = \"Vote for DPP Candidate Tsai Ing-wen\")\n\n\n\n\n\nCode\nggplot(data = TEDS_2016) +\n  geom_boxplot(mapping = aes(x = votetsai, y = age), fill = \"indianred4\") +\n  coord_flip()\n\n\n\n\n\nCode\nggplot(data = TEDS_2016) +\n  geom_boxplot(mapping = aes(x = votetsai, y = income), fill = \"indianred4\") +\n  coord_flip()\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = votetsai, fill = factor(Taiwanese))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"steelblue4\", \"1\" = \"indianred4\"))\n\n\n\n\n\nCode\nggplot(TEDS_2016, aes(x = edu, fill = factor(votetsai))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"No\" = \"steelblue4\", \"Yes\" = \"indianred4\"))"
  },
  {
    "objectID": "posts/6323_07.html",
    "href": "posts/6323_07.html",
    "title": "EPPS 6323: Assignment 6 Linear regression",
    "section": "",
    "text": "Requirement(s) of LDA\n\nIndependence of predictors within each class\nNormal distribution or predictors within each class\n\nHow LDA is different from Logistic Regression?\nLDA is more restrictive in its assumptions about the data distribution and independence of predictors, whereas logistic regression is more flexible and can handle a wider range of data distributions.\nROC (Receiver Operating Characteristic)\nROC is a graphical representation of the performance of a binary classification model across different threshold values. It plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. It’s commonly used to evaluate the performance of classification algorithms, especially when the class distribution is imbalanced.\nSensitivity and specificity\nSensitivity: Sensitivity, also known as the true positive rate, measures the proportion of actual positive cases that are correctly identified by the classifier.\nSpecificity: Specificity measures the proportion of actual negative cases that are correctly identified by the classifier.\nThe choice on which one should be prioritized depends on the domain.\nPrediction error rate\n(False positive + false negative)/ Total cases"
  },
  {
    "objectID": "posts/6323_07.html#assignment",
    "href": "posts/6323_07.html#assignment",
    "title": "EPPS 6323: Assignment 6 Linear regression",
    "section": "",
    "text": "Requirement(s) of LDA\n\nIndependence of predictors within each class\nNormal distribution or predictors within each class\n\nHow LDA is different from Logistic Regression?\nLDA is more restrictive in its assumptions about the data distribution and independence of predictors, whereas logistic regression is more flexible and can handle a wider range of data distributions.\nROC (Receiver Operating Characteristic)\nROC is a graphical representation of the performance of a binary classification model across different threshold values. It plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. It’s commonly used to evaluate the performance of classification algorithms, especially when the class distribution is imbalanced.\nSensitivity and specificity\nSensitivity: Sensitivity, also known as the true positive rate, measures the proportion of actual positive cases that are correctly identified by the classifier.\nSpecificity: Specificity measures the proportion of actual negative cases that are correctly identified by the classifier.\nThe choice on which one should be prioritized depends on the domain.\nPrediction error rate\n(False positive + false negative)/ Total cases"
  },
  {
    "objectID": "posts/6323_07.html#lab",
    "href": "posts/6323_07.html#lab",
    "title": "EPPS 6323: Assignment 6 Linear regression",
    "section": "Lab",
    "text": "Lab\n\n# explore dataset\nlibrary (ISLR)\n\nWarning: package 'ISLR' was built under R version 4.3.3\n\nattach(Smarket)\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\n\n\nLogistic Regression\n\n# Create a dataframe for data browsing\nsm=Smarket\n\n# Bivariate Plot of inter-lag correlations\npairs(Smarket,col=Smarket$Direction,cex=.5, pch=20)\n\n\n\n# Logistic regression\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\nglm.probs=predict(glm.fit,type=\"response\") \nglm.probs[1:5]\n\n        1         2         3         4         5 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 \n\nglm.pred=ifelse(glm.probs&gt;0.5,\"Up\",\"Down\")\nattach(Smarket)\n\nThe following objects are masked from Smarket (pos = 3):\n\n    Direction, Lag1, Lag2, Lag3, Lag4, Lag5, Today, Volume, Year\n\ntable(glm.pred,Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\nmean(glm.pred==Direction)\n\n[1] 0.5216\n\n# Make training and test set for prediction\ntrain = Year&lt;2005\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs &gt;0.5,\"Up\",\"Down\")\nDirection.2005=Smarket$Direction[!train]\ntable(glm.pred,Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\nmean(glm.pred==Direction.2005)\n\n[1] 0.4801587\n\n#Fit smaller model\nglm.fit=glm(Direction~Lag1+Lag2,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs &gt;0.5,\"Up\",\"Down\")\ntable(glm.pred,Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred==Direction.2005)\n\n[1] 0.5595238\n\n# Check accuracy rate\n106/(76+106)\n\n[1] 0.5824176"
  },
  {
    "objectID": "posts/6323_04.html",
    "href": "posts/6323_04.html",
    "title": "EPPS 6323: Lab04 R programming (Unsupervised learning)",
    "section": "",
    "text": "Unsupervised learning is a class of machine learning algorithms to identify patterns or grouping structure in the data. Unlike supervised learning which relies on “supervised” information such as the dependent variable to guide modeling, unsupervised learning seeks to explore the structure and possible groupings of unlabeled data. This information will be useful to provide pre-processor for supervised learning.\nUnsupervised learning has no explicit dependent variable of Y for prediction. Instead, the goal is to discover interesting patterns about the measurements on \\((X_{1}), (X_{2}), . . . , (X_{p})\\) and identify any subgroups among the observations.\nGenerally, in this section, the two general methods are introduced: Principal components analysis and Clustering.\n\n\nPrincipal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.\n\n\n\n\n\nThe K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
  },
  {
    "objectID": "posts/6323_04.html#principal-component-analysis-pca",
    "href": "posts/6323_04.html#principal-component-analysis-pca",
    "title": "EPPS 6323: Lab04 R programming (Unsupervised learning)",
    "section": "",
    "text": "Principal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance."
  },
  {
    "objectID": "posts/6323_04.html#clustering",
    "href": "posts/6323_04.html#clustering",
    "title": "EPPS 6323: Lab04 R programming (Unsupervised learning)",
    "section": "",
    "text": "The K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
  },
  {
    "objectID": "posts/6323_02.html",
    "href": "posts/6323_02.html",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "",
    "text": "Lab01: R programming basics I\nLab02: R programming basics II\nAssignment"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Knowledge Mining",
    "section": "",
    "text": "Final Project\n  \n  \n    \n     Final Paper\n  \n  \n    \n     Project Proposal"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Knowledge Mining",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1\nAssignment 2\nAssignment 3\nAssignment 4\nAssignment 5\nAssignment 6\nAssignment 7\nAssignment 8"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/6323_01.html",
    "href": "posts/6323_01.html",
    "title": "EPPS 6323: Introduction",
    "section": "",
    "text": "Data or research question\nPatterns and trends in health misinformation\nDo you have a hypothesis?\nHealth misinformation exhibits distinct thematic patterns, with certain words and topics recurring more frequently than others.\nHow data can be collected and stored?\nOnline databases on health misinformation\nWhat methods could be considered?\nUnsupervised learning methods - NLP tools, sentiment analysis"
  },
  {
    "objectID": "posts/6323_03.html",
    "href": "posts/6323_03.html",
    "title": "EPPS 6323: Lab02 R programming (Exploratory Data Analysis)",
    "section": "",
    "text": "R Programming (EDA)\n\n## Creating a function: regplot\n## Combine the lm, plot and abline functions to create a regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y)\n  abline(fit,col=\"red\")\n}\n\n\nattach(ISLR::Carseats)\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}  # \"...\" is called ellipsis, which is designed to take any number of named or unnamed arguments.\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n\n\n\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\nRegression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\nsummary(petal_lm)\n\n\nCall:\nlm(formula = Petal.Length ~ 0 + Sepal.Length + Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.70623 -0.51867 -0.08334  0.49844  1.93093 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nSepal.Length  1.56030    0.04557   34.24   &lt;2e-16 ***\nSepal.Width  -1.74570    0.08709  -20.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 148 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9726 \nF-statistic:  2663 on 2 and 148 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/6323_06.html",
    "href": "posts/6323_06.html",
    "title": "EPPS 6323: Assignment 6 Linear regression",
    "section": "",
    "text": "Lab\n\nLoad datasets from MASS and ISLR packages\n\nattach(Boston)\n\n\n\nSimple linear regression\n\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# What is the Boston dataset?\n?Boston\nplot(medv~lstat,Boston, pch=20, cex=.8, col=\"steelblue\")\nfit1=lm(medv~lstat,data=Boston)\nfit1\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(fit1)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nabline(fit1,col=\"firebrick\")\n\n\n\nnames(fit1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nconfint(fit1) # confidence intervals\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\n\n\nPredictions using values in lstat\n\n# Predictions using values in lstat\npredict(fit1,data.frame(lstat=c(0,5,10,15)),interval=\"confidence\") # confidence intervals\n\n       fit      lwr      upr\n1 34.55384 33.44846 35.65922\n2 29.80359 29.00741 30.59978\n3 25.05335 24.47413 25.63256\n4 20.30310 19.73159 20.87461\n\npredict(fit1,data.frame(lstat=c(0,5,10,15)),interval=\"prediction\") # prediction intervals\n\n       fit       lwr      upr\n1 34.55384 22.291923 46.81576\n2 29.80359 17.565675 42.04151\n3 25.05335 12.827626 37.27907\n4 20.30310  8.077742 32.52846\n\n\nPrediction interval uses sample mean and takes into account the variability of the estimators for μ and σ.\nTherefore, the interval will be wider.\n\n\nMultiple linear regression\n\nfit2=lm(medv~lstat+age,data=Boston)\nsummary(fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nfit3=lm(medv~.,Boston)\nsummary(fit3)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(2,2))\nplot(fit3,pch=20, cex=.8, col=\"steelblue\")\nmtext(\"fit3\", side = 3, line = - 2, cex = 2, outer = TRUE)\n\n\n\n# Update function to re-specify the model, i.e. include all but age and indus variables\nfit4=update(fit3,~.-age-indus)\nsummary(fit4)\n\n\nCall:\nlm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + black + lstat, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***\ncrim         -0.108413   0.032779  -3.307 0.001010 ** \nzn            0.045845   0.013523   3.390 0.000754 ***\nchas          2.718716   0.854240   3.183 0.001551 ** \nnox         -17.376023   3.535243  -4.915 1.21e-06 ***\nrm            3.801579   0.406316   9.356  &lt; 2e-16 ***\ndis          -1.492711   0.185731  -8.037 6.84e-15 ***\nrad           0.299608   0.063402   4.726 3.00e-06 ***\ntax          -0.011778   0.003372  -3.493 0.000521 ***\nptratio      -0.946525   0.129066  -7.334 9.24e-13 ***\nblack         0.009291   0.002674   3.475 0.000557 ***\nlstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n# Set the next plot configuration\npar(mfrow=c(2,2), main=\"fit4\")\nplot(fit4,pch=20, cex=.8, col=\"steelblue\")\nmtext(\"fit4\", side = 3, line = - 2, cex = 2, outer = TRUE)\n\n\n\n# Uses coefplot to plot coefficients.  Note the line at 0.\npar(mfrow=c(1,1))\narm::coefplot(fit4)\n\n\n\n### Nonlinear terms and Interactions\nfit5=lm(medv~lstat*age,Boston) # include both variables and the interaction term x1:x2\nsummary(fit5)\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n## I() identity function for squared term to interpret as-is\n## Combine two command lines with semicolon\nfit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,1))\nplot(medv~lstat, pch=20, col=\"forestgreen\")\n\npoints(lstat,fitted(fit6),col=\"firebrick\",pch=20)\nfit7=lm(medv~poly(lstat,4))\npoints(lstat,fitted(fit7),col=\"steelblue\",pch=20)\n\n\n\n\n\n\nQualitative predictors\n\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nsummary(Carseats)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\nfit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats) # add two interaction terms\nsummary(fit1)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Age:Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(Carseats$ShelveLoc) # what is contrasts function?\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n?contrasts\n\n### Writing an R function to combine the lm, plot and abline functions to \n### create a one step regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y, pch=20)\n  abline(fit,col=\"firebrick\")\n}\nattach(Carseats)\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"firebrick\")\n}\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"steelblue\",pch=20)\n\n\n\n\n\n\n\nUsing the TEDS2016 dataset\n\nlibrary(haven)\nTEDS_2016 &lt;-read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nglm.vt=glm(votetsai~female, data=TEDS_2016,family=binomial)\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female, family = binomial, data = TEDS_2016)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.54971    0.08245   6.667 2.61e-11 ***\nfemale      -0.06517    0.11644  -0.560    0.576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1666.5  on 1260  degrees of freedom\nResidual deviance: 1666.2  on 1259  degrees of freedom\n  (429 observations deleted due to missingness)\nAIC: 1670.2\n\nNumber of Fisher Scoring iterations: 4\n\n\nFemale voters are less likely to vote for President Tsai because of the negative coefficient but result is not significant.\n\nAdding Variables\n\nglm.vt2=glm(votetsai~female+KMT+DPP+edu+income+age, data=TEDS_2016,family=binomial)\nsummary(glm.vt2)\n\n\nCall:\nglm(formula = votetsai ~ female + KMT + DPP + edu + income + \n    age, family = binomial, data = TEDS_2016)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.618640   0.592084   2.734  0.00626 ** \nfemale       0.047406   0.177403   0.267  0.78930    \nKMT         -3.156273   0.250360 -12.607  &lt; 2e-16 ***\nDPP          2.888943   0.267968  10.781  &lt; 2e-16 ***\nedu         -0.184604   0.083102  -2.221  0.02632 *  \nincome       0.013727   0.034382   0.399  0.68971    \nage         -0.011808   0.007164  -1.648  0.09931 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  836.15  on 1250  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 850.15\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nplot(glm.vt2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nglm.vt3=glm(votetsai~female+KMT+DPP+edu+income+age+Econ_worse+Govt_dont_care+Taiwanese+Independence+Minnan_father, data=TEDS_2016,family=binomial)\nsummary(glm.vt3)\n\n\nCall:\nglm(formula = votetsai ~ female + KMT + DPP + edu + income + \n    age + Econ_worse + Govt_dont_care + Taiwanese + Independence + \n    Minnan_father, family = binomial, data = TEDS_2016)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -0.0387221  0.6713948  -0.058   0.9540    \nfemale         -0.0693281  0.1883495  -0.368   0.7128    \nKMT            -2.9264019  0.2576310 -11.359  &lt; 2e-16 ***\nDPP             2.5026466  0.2748843   9.104  &lt; 2e-16 ***\nedu            -0.1447022  0.0875728  -1.652   0.0985 .  \nincome          0.0228238  0.0363006   0.629   0.5295    \nage            -0.0008739  0.0076362  -0.114   0.9089    \nEcon_worse      0.2940295  0.1878949   1.565   0.1176    \nGovt_dont_care -0.0571474  0.1869879  -0.306   0.7599    \nTaiwanese       0.9422408  0.1972915   4.776 1.79e-06 ***\nIndependence    1.0389008  0.2509717   4.140 3.48e-05 ***\nMinnan_father   0.1318885  0.2102126   0.627   0.5304    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  774.99  on 1245  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 798.99\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "posts/6323_08.html",
    "href": "posts/6323_08.html",
    "title": "Knowledge Mining",
    "section": "",
    "text": "Knowledge Mining: Assignment 8\n\n\nAssignment Exercise\n\nFrom the three methods (best subset, forward stepwise, and backward stepwise):\na. Which of the three models with k predictors has the smallest training RSS?\nBest subset selection has the smallest training RSS as it fits every combination of predictors while backside and forward stepwise models depend on which predictors they pick first.\nb. Which of the three models with k predictors has the smallest test RSS?\nBest subset may have the smallest test RSS because it considers more models. However it also may be overfitting the training data, and one of the other models may find a better model for the test data.\nGenerate simulated data, and then use this data to perform best subset selection.\ni. Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector 𝜀 of length n = 100.\nii. Generate a response vector 𝑦 of length n = 100 according to the model:\n\n\nset.seed(1) \nX=rnorm(100) \neps=rnorm(100)   \nbeta0=4 \nbeta1=9 \nbeta2=2 \nbeta3=1  \nY=beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps \n\nUse the regsubsets() function from the leaps package to perform best subset selection in order to choose the best model containing the predictors. 𝑥,𝑥2…𝑥10.\nWhat is the best model obtained according to Cp, BIC, and adjusted 𝑅2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both 𝑥 and 𝑦 .\n\nlibrary(leaps)\ndata.full &lt;- data.frame(y = Y, x = X)\nmod.full &lt;- regsubsets(y ~ poly(x, 10, raw = TRUE), data = data.full, nvmax = 10)\nmod.summary &lt;- summary(mod.full)\nmin.cp &lt;- which.min(mod.summary$cp)\nmin.bic &lt;- which.min(mod.summary$bic)\nmax.adjr2 &lt;- which.max(mod.summary$adjr2)\n\nplot(mod.summary$cp, xlab = \"Subset Size\", ylab = \"Cp\", pch = 20, type = \"l\")\npoints(min.cp, mod.summary$cp[min.cp], pch = 4, col = \"red\", lwd = 7)\n\n\n\nplot(mod.summary$bic, xlab = \"Subset Size\", ylab = \"BIC\", pch = 20, type = \"l\")\npoints(min.bic, mod.summary$bic[min.bic], pch = 4, col = \"red\", lwd = 7)\n\n\n\nplot(mod.summary$adjr2, xlab = \"Subset Size\", ylab = \"adjr2\", pch = 20, type = \"l\")\npoints(max.adjr2, mod.summary$adjr2[max.adjr2], pch = 4, col = \"red\", lwd = 7)\n\n\n\n\nBest model obtained according to Cp = 4\nBIC = 3\nadjusted 𝑅2 = 4\n\n# Coefficient for 3-variable model \ncoefficients(mod.full, id = 3)  \n\n             (Intercept) poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 \n                4.061507                 8.975280                 1.876209 \npoly(x, 10, raw = TRUE)3 \n                1.017639 \n\n# Coefficient for 4-variable model \ncoefficients(mod.full, id = 4)\n\n             (Intercept) poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 \n              4.07200775               9.38745596               1.84575641 \npoly(x, 10, raw = TRUE)3 poly(x, 10, raw = TRUE)5 \n              0.55797426               0.08072292 \n\n\nRepeat 3, using forward stepwise selection and using backwards stepwise selection. How does your answer compare to the results in 3?\n\nmod.fwd &lt;- regsubsets(y ~ poly(x, 10, raw = TRUE), data = data.full, nvmax = 10, method = \"forward\")\nmod.bwd &lt;- regsubsets(y ~ poly(x, 10, raw = TRUE), data = data.full, nvmax = 10, method = \"backward\")\nfwd.summary &lt;- summary(mod.fwd)\nbwd.summary &lt;- summary(mod.bwd)\n\nmin.cp.f &lt;- which.min(fwd.summary$cp)\nmin.bic.f &lt;- which.min(fwd.summary$bic)\nmax.adjr2.f &lt;- which.max(fwd.summary$adjr2)\n\nmin.cp.b &lt;- which.min(bwd.summary$cp)\nmin.bic.b &lt;- which.min(bwd.summary$bic)\nmax.adjr2.b &lt;- which.max(bwd.summary$adjr2)\n\npar(mfrow = c(3, 2))\n\nplot(fwd.summary$cp, xlab = \"Subset Size\", ylab = \"Fwd Cp\", pch = 20, type = \"l\")\npoints(min.cp.f, fwd.summary$cp[min.cp.f], pch = 4, col = \"red\", lwd = 7)\n\nplot(bwd.summary$cp, xlab = \"Subset Size\", ylab = \"Bwd Cp\", pch = 20, type = \"l\")\npoints(min.cp.b, bwd.summary$cp[min.cp.b], pch = 4, col = \"red\", lwd = 7)\n\nplot(fwd.summary$bic, xlab = \"Subset Size\", ylab = \"Fwd BIC\", pch = 20, type = \"l\")\npoints(min.bic.f, fwd.summary$bic[min.bic.f], pch = 4, col = \"red\", lwd = 7)\n\nplot(bwd.summary$bic, xlab = \"Subset Size\", ylab = \"Bwd BIC\", pch = 20, type = \"l\")\npoints(min.bic.b, bwd.summary$bic[min.bic.b], pch = 4, col = \"red\", lwd = 7)\n\nplot(fwd.summary$adjr2, xlab = \"Subset Size\", ylab = \"Fwd adjr2\", pch = 20, type = \"l\")\npoints(max.adjr2.f, fwd.summary$adjr2[max.adjr2.f], pch = 4, col = \"red\", lwd = 7)\n\nplot(bwd.summary$adjr2, xlab = \"Subset Size\", ylab = \"Bwd adjr2\", pch = 20, type = \"l\")\npoints(max.adjr2.b, bwd.summary$adjr2[max.adjr2.b], pch = 4, col = \"red\", lwd = 7)\n\n\n\n\nFor forward stepwise, Best model obtained according to Cp = 5\nBIC = 4\nadjusted 𝑅2 = 5\nFor backward stepwise, Best model obtained according to Cp = 6\nBIC = 6\nadjusted 𝑅2 = 8\n\nFile: Lab_LDA01.R\nTheme: Linear Discriminant Analysis\nAdapted from ISLR Chapter 4 Lab\n\n# Setup\nrequire(ISLR)\nrequire(MASS)\nlibrary(descr)\nattach(Smarket)\n\n\nfreq(Direction)\n\n\n\n\nDirection \n      Frequency Percent\nDown        602   48.16\nUp          648   51.84\nTotal      1250  100.00\n\ntrain = Year&lt;2005\nlda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year&lt;2005)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year &lt; \n    2005)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit, col=\"dodgerblue\")\n\n\n\n\n\nSmarket.2005=subset(Smarket,Year==2005) # Creating subset with 2005 data for prediction\nlda.pred=predict(lda.fit,Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\nlda.class=lda.pred$class\nDirection.2005=Smarket$Direction[!train] \ntable(lda.class,Direction.2005) \n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\ndata.frame(lda.pred)[1:5,]\n\n     class posterior.Down posterior.Up         LD1\n999     Up      0.4901792    0.5098208  0.08293096\n1000    Up      0.4792185    0.5207815  0.59114102\n1001    Up      0.4668185    0.5331815  1.16723063\n1002    Up      0.4740011    0.5259989  0.83335022\n1003    Up      0.4927877    0.5072123 -0.03792892\n\ntable(lda.pred$class,Smarket.2005$Direction)\n\n      \n       Down  Up\n  Down   35  35\n  Up     76 106\n\nmean(lda.pred$class==Smarket.2005$Direction)\n\n[1] 0.5595238"
  },
  {
    "objectID": "posts/Lab01.html",
    "href": "posts/Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"has_annotations\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9934444\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "posts/Lab01.html#create-object-using-the-assignment-operator--",
    "href": "posts/Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "posts/Lab01.html#using-function",
    "href": "posts/Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "posts/Lab01.html#using---operators",
    "href": "posts/Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"has_annotations\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "posts/Lab01.html#matrix-operations",
    "href": "posts/Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9934444\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "posts/Lab01.html#simple-descriptive-statistics-base",
    "href": "posts/Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "posts/Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "posts/Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  }
]